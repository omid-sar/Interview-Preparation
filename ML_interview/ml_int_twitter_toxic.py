# -*- coding: utf-8 -*-
"""ML_int_twitter_toxic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/omid-sar/Interview-Preparation/blob/main/ML_interview/ML_int_twitter_toxic.ipynb
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

!wget https://raw.githubusercontent.com/omid-sar/Project_Data_Hub/main/twitter_toxic_final_balanced_dataset.csv

import pandas as pd
df_org = pd.read_csv("twitter_toxic_final_balanced_dataset.csv", usecols=["Toxicity", "tweet"])
df_org.head()

for _ in range(10):
  rand = np.random.randint(len(df_org))
  print(df_org.iloc[rand]["tweet"], df_org.iloc[rand]["Toxicity"])

import matplotlib.pyplot as plt

df = df_org.copy()

df["tweet_len"] = [len(seq.split()) for seq in df_org["tweet"]]
toxic_len = df[df["Toxicity"] == 0]["tweet_len"]
norm_len = df[df["Toxicity"] == 1]["tweet_len"]

print("MAX LENGTH:" ,max(df["tweet_len"]))

fig , (ax1, ax2) = plt.subplots(1, 2, figsize=(10,6))
ax1.hist(toxic_len, bins=100, alpha=0.5, label="TOXIC")
ax1.hist(norm_len, bins=100, alpha=0.5, label="Neutral")
ax1.legend()

ax2.boxplot([toxic_len, norm_len], labels=["TOXIC", "Neutral"])
plt.show()

X = df["tweet"].values
y = df["Toxicity"].values
sequences = [sequence for sequence in X]

from transformers import DistilBertTokenizer, BertTokenizer

#distilled_tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

MAX_LEN = 32

model_inputs = tokenizer(sequences, padding=True, truncation=True, max_length=MAX_LEN ,return_tensors='pt')
model_inputs

print("tweet: \n " , sequences[0],
      "\n input_ids: \n",  model_inputs.input_ids[0],
      "\n token_type_ids: \n" , model_inputs.token_type_ids[0],
      "\n attention_mask: \n" , model_inputs.attention_mask[0])

import torch
from torch.utils.data import DataLoader, TensorDataset

BATCH_SIZE = 16
NUM_WORKERS = 2
labels = torch.tensor(y)
dataset = TensorDataset(model_inputs["input_ids"], model_inputs["attention_mask"], labels)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS , pin_memory=True)

from torch.utils.data import random_split

train_size = int(len(dataset)* 0.7)
val_size = int(len(dataset)* 0.2)
test_size = len(dataset) - train_size - val_size

train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

import torch.nn as nn


class LSTM(nn.Module):

  def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1, num_classes=2,
               bidirectional=True, f_c_layers=[128,256]):
    super(LSTM, self).__init__()

    self.hidden_size = hidden_size
    self.num_layers = num_layers
    self.bidirectional = bidirectional
    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)
    self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers,
                        batch_first=True, bidirectional=bidirectional)

    # The input size for the first fully connected layer
    fc_input_size = 2 * hidden_size if bidirectional==True else hidden_size

    self.fc1 = nn.Linear(fc_input_size, f_c_layers[0])
    self.fc2 = nn.Linear(f_c_layers[0], f_c_layers[1])
    self.fc3 = nn.Linear(f_c_layers[1], num_classes)
    self.relu = nn.ReLU()
    self.sigmoid = nn.Sigmoid()

  def forward(self,x):
    x = self.embedding(x)

    # Initialize hidden and cell states
    h0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_size).to(x.device)
    c0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_size).to(x.device)

    out, _ = self.lstm(x, (h0, c0))

    # Take the output from the last time step (batch_size, seq_length, num_directions * hidden_size)
    out = out[:, -1, :]

    out = self.relu(self.fc1(out))
    out = self.relu(self.fc2(out))
    out = self.sigmoid(self.fc3(out))

    return out

VOCAB_SIZE = tokenizer.vocab_size
EMBEDDING_DIM = 50
HIDDEN_SIZE = 128
FC1_SIZE = 256
FC2_SIZE = 128

